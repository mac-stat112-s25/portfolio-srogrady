[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "Best Univariate Viz",
    "section": "",
    "text": "Use this file to generate a professional looking univariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(ggplot2)\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\n\nCodeggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 50) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\", title = \"Looking at Number of Hikes at different Elevations\", caption = \"Made by Shaylee (4/25)\")\n\n\n\n\n\n\nCodefig.alt = \"Histogram with x-axis showing ranges of hike elevations ranging from 3820-5344\"",
    "crumbs": [
      "Best Work",
      "Best Univariate Viz"
    ]
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "Best Bivariate Viz",
    "section": "",
    "text": "Use this file to generate a professional looking bivariate visualization. The visualization will not perfect the first time but you are expected to improve it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(ggplot2)\n# Import data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n\n\nCodeggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point(color = \"light blue\" ) + \n  geom_smooth()+\n  theme_classic()+\n  labs(x = \"Median Age (year)\", y = \"The 2020 Republican Country-level Support\", title = \"Relationship between Median Age and 2020 Republican Election Suppport\", caption = \"Made by Shaylee (4/25)\")\n\n\n\n\n\n\nCodefig.alt = \"Scatter Plot showing the Relationship between Median Age (x-axis) and 2020 Republican Election Suppport (y-axis). Use a line of best fit to show a possible weak positive relationship where most of the data is clustered between age 30-50 and 25-75% support.\"",
    "crumbs": [
      "Best Work",
      "Best Bivariate Viz"
    ]
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "Best Trivariate Viz",
    "section": "",
    "text": "Use this file to generate a professional looking trivariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(tidyverse)\nlibrary(lubridate)\n\n# Import the data\n# Only keep certain variables of interest\ntrips &lt;- readRDS(gzcon(url(\"https://mac-stat.github.io/data/2014-Q4-Trips-History-Data-Small.rds\"))) %&gt;% \n  select(client, sstation, sdate, duration) %&gt;% \n  mutate(duration = as.numeric(hms(duration))/60)\n\n\n\nCodetrips &lt;- trips |&gt; \n  mutate(\n    s_date = as_date(sdate),\n    s_day_of_week = wday(sdate,label=TRUE),\n    s_hour = hour(sdate),\n    s_minute = minute(sdate),\n    s_time_of_day = s_hour + s_minute/60)\n\n\n\nCodeggplot(trips,aes(x=s_time_of_day, color=client))+\n  geom_density()+\n  facet_wrap(~s_day_of_week)+\n  theme_classic()+\n  labs(titel =\"times of day *and* days of the week registered vs client rides\", x = \"Time of Day\", caption = \"Made by Shaylee (4/25)\")\n\n\n\n\n\n\nCodefig.alt = \"density plot of the time of day of bike rides faceted by the day of the week and colored in blue and red by whether or not the biker is a casual biker or registered.\"",
    "crumbs": [
      "Best Work",
      "Best Trivariate Viz"
    ]
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "Best Quadvariate Viz",
    "section": "",
    "text": "Use this file to generate a professional looking quadvariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(mosaic)\nlibrary(lubridate)\ndata(\"Birthdays\")\n\ndaily_births &lt;- Birthdays |&gt;\n  group_by(date) |&gt;\n    summarize(total = sum(births))\n\ndaily_births &lt;- daily_births |&gt;\n  mutate(week_day = wday(date,label = TRUE)) |&gt; \n  mutate(month_day = mday(date)) |&gt; \n  mutate(year = year(date))\n  \n\nholidays &lt;- read.csv(\"https://mac-stat.github.io/data/US-Holidays.csv\") |&gt;\n  mutate(date = as.POSIXct(lubridate::dmy(date)))\n\n\n\nCode# Define daily_births_1980\ndaily_births_1980 &lt;- daily_births |&gt;\n  filter(year == 1980) |&gt;\n  left_join(holidays, by = \"date\") |&gt;\n  mutate(is_holiday = !is.na(holiday))\n\n\n\nCodedaily_births_1980 |&gt;\n  ggplot(aes(y=total, x = date, color = week_day, shape = is_holiday))+\n    geom_point()+\n  theme_classic()+\n  labs(title = \"Babies Born in 1980\", y = \"Total Number of Babies Born\", x = \"Month\", caption = \"Made by Shaylee (4/25)\")\n\n\n\n\n\n\nCodefig.alt = \"dot plot representing the total number of babies born (y-axis) per day (x-axis) in 1980. Color each date according to its day of the week, and shape each date according to whether or not it’s a holiday..\"\n\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Shaylee O'Grady]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar-title\"}\n[COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar-title\"}\n[Best Spatial Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-next\"}\n[Best Trivariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-prev\"}\n[Welcome]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/index.htmlWelcome\"}\n[Best Work]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"}\n[Best Univariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-uni.htmlBest-Univariate-Viz\"}\n[Best Bivariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-bi.htmlBest-Bivariate-Viz\"}\n[Best Trivariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-tri.htmlBest-Trivariate-Viz\"}\n[Best Quadvariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-quad.htmlBest-Quadvariate-Viz\"}\n[Best Spatial Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-spatial.htmlBest-Spatial-Viz\"}\n[Solo Project]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/Solo Project.htmlSolo-Project\"}\n[In-class Activities]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"}\n[My first Quarto document]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/quarto-demo.htmlMy-first-Quarto-document\"}\n[Univariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-uni.htmlUnivariate-Viz\"}\n[Bivariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-bi.htmlBivariate-Viz\"}\n[Multivariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-multi.htmlMultivariate-Viz\"}\n[Spatial Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-spatial.htmlSpatial-Viz\"}\n[Effective Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/Effectiveica.htmlEffective-Viz\"}\n[Wrangling]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/wranglingICA.htmlWrangling\"}\n[Dates]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/datesICA.htmlDates\"}\n[Reshaping]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ICAreshapingg.htmlReshaping\"}\n[Joining]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/joining.htmlJoining\"}\n[Factors]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/Factors.htmlFactors\"}\n[Strings]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/strings.htmlStrings\"}\n[Data Import]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/Data_Import.htmlData-Import\"}\n[Exams]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-3\"}\n[Exam 1]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/test/Exam1.htmlExam-1\"}\n[Exam 2]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/test/Exam2.htmlExam-2\"}\n[Summary]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-4\"}\n[Sheet 1]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/test/Sheet1.htmlSheet-1\"}\n[Sheet 2]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/test/Sheet2.htmlSheet-2\"}\n[Best Work]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-breadcrumbs-Best-Work\"}\n[Best Quadvariate Viz]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-breadcrumbs-Best-Quadvariate-Viz\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[Best Quadvariate Viz – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metatitle\"}\n[Best Quadvariate Viz – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercardtitle\"}\n[Best Quadvariate Viz – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardtitle\"}\n[COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metasitename\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercarddesc\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n# Best Quadvariate Viz {-}\n\n\nUse this file to generate a professional looking **quadvariate** visualization.  The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nquarto-executable-code-5450563D\n\n```r\nlibrary(mosaic)\nlibrary(lubridate)\ndata(\"Birthdays\")\n\ndaily_births &lt;- Birthdays |&gt;\n  group_by(date) |&gt;\n    summarize(total = sum(births))\n\ndaily_births &lt;- daily_births |&gt;\n  mutate(week_day = wday(date,label = TRUE)) |&gt; \n  mutate(month_day = mday(date)) |&gt; \n  mutate(year = year(date))\n  \n\nholidays &lt;- read.csv(\"https://mac-stat.github.io/data/US-Holidays.csv\") |&gt;\n  mutate(date = as.POSIXct(lubridate::dmy(date)))\n\nquarto-executable-code-5450563D\n# Define daily_births_1980\ndaily_births_1980 &lt;- daily_births |&gt;\n  filter(year == 1980) |&gt;\n  left_join(holidays, by = \"date\") |&gt;\n  mutate(is_holiday = !is.na(holiday))\nquarto-executable-code-5450563D\ndaily_births_1980 |&gt;\n  ggplot(aes(y=total, x = date, color = week_day, shape = is_holiday))+\n    geom_point()+\n  theme_classic()+\n  labs(title = \"Babies Born in 1980\", y = \"Total Number of Babies Born\", x = \"Month\", caption = \"Made by Shaylee (4/25)\")\nfig.alt = \"dot plot representing the total number of babies born (y-axis) per day (x-axis) in 1980. Color each date according to its day of the week, and shape each date according to whether or not it’s a holiday..\"\n\n:::",
    "crumbs": [
      "Best Work",
      "Best Quadvariate Viz"
    ]
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "Best Spatial Viz",
    "section": "",
    "text": "Use this file to generate a professional looking spatial visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(leaflet)\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nstarbucks_mn &lt;- starbucks |&gt;   \n  filter(Country == \"US\", State.Province == \"MN\")\n\n\n\nCodeleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(color =\"red\", lng = ~Longitude, lat = ~Latitude)",
    "crumbs": [
      "Best Work",
      "Best Spatial Viz"
    ]
  },
  {
    "objectID": "bw/Solo Project.html",
    "href": "bw/Solo Project.html",
    "title": "Solo Project",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(readr)\nlibrary(RColorBrewer)\n\n\n\nCodenz_boundaries &lt;- ne_states(country = \"new zealand\")\n\n\n\nCodest_bbox(nz_boundaries)\n\n       xmin        ymin        xmax        ymax \n-177.957997  -52.600313  178.843923   -8.543227 \n\n\nFirst step was to map New Zealand and outline its 16 regions within that map so that I could investigate each region later on.\n\nCodelibrary(ggplot2)\n  ggplot(data = nz_boundaries) +\n    geom_sf( ) +\n  theme_classic()+\n    coord_sf(xlim = c(160, 180), ylim = c(-52.600313, -32), expand = FALSE)\n\n\n\n\n\n\n\n\nCodenz_boundaries2 &lt;- nz_boundaries |&gt;\n  filter(name %in% c(\"Southland\", \"Marlborough District\", \"Nelson City\", \"Tasman District\", \"West Coast\",  \"Otago\", \"Canterbury\",\"Auckland\", \"Waikato\", \"Wellington\", \"Manawatu-Wanganui\", \"Taranaki\", \"Northland\", \"Bay of Plenty\", \"Gisborne District\",\"Hawke's Bay\"))\n\n# these are the true 16 regions for local government that New Zealand is divided up by. \n\n\nSecond step I filled the map by the region name to seperate each of the regions by a color.\n\nCodelibrary(ggplot2)\n\nggplot(data = nz_boundaries2, aes(fill = name)) +\n    geom_sf( ) +\n  theme_classic()+\n    coord_sf(xlim = c(160, 180), ylim = c(-52.600313, -32))\n\n\n\n\n\n\n\nThen I used plotly to make the map interactive so I could hover to know what each region is named easily.\n\nCodelibrary(ggplot2)\nnz &lt;- ggplot(data = nz_boundaries2, aes(fill = name)) +\n    geom_sf(color = \"black\", size = 0.05) +\n    theme_classic()+\n    coord_sf(xlim = c(165, 180), ylim = c(-49, -32))+\n    scale_fill_viridis_d(option = \"plasma\")\n               \nlibrary(plotly)\n\nggplotly(nz, tooltip = \"fill\")\n\n\n\n\nCode#made it interactive to be able to know what region I am looking at using plotly.\n\n\n\nCoderegion &lt;- c(\"Southland\", \"Marlborough District\", \"Nelson City\", \"Tasman District\", \"West Coast\",  \"Otago\", \"Canterbury\",\"Auckland\", \"Waikato\", \"Wellington\", \"Manawatu-Wanganui\", \"Taranaki\", \"Northland\", \"Bay of Plenty\", \"Gisborne District\",\"Hawke's Bay\")\n\nyear &lt;- c(2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023,2023)\n\nmedian_age &lt;- c(39.1, 46.1, 44.0, 46.8, 48.1, 38.4,39.1, 35.9, 37.9, 37.9, 41.0, 40.4, 43.2, 39.7, 36.7, 40.4)\n\n# then I had to combine this data into a dataset\n\nnz_census &lt;- data.frame(region, year, median_age)\n\n\n\nCodenz_map_data &lt;- nz_boundaries2 |&gt;\n  left_join(nz_census, by = c(\"name\" = \"region\"))\n\n\nThen I made choropleth map of New Zealand regions Median age in 2023 using data from the website infometrics that held New Zealand 2023 census data “https://rep.infometrics.co.nz/new-zealand/census/drill-down/five-year-age-group/age-median?census=gisborne-district”\n\nCodeggplot(nz_map_data) +\n  geom_sf(aes(fill = median_age), color = \"white\") +\n  scale_fill_viridis_c(option = \"C\", direction = -1, name = \"Median Age\") +\n  theme_void() +\n  labs(title = \"Median Age by New Zealand Region in 2023\", caption = \"Made by Shaylee (4/25); Data from 2023 NZ Census\")\n\n\n\n\n\n\nCodefig.alt = \"Choropleth map showing New Zealands median age for each of it's 16 main regions based on it's 2023 Census data.\"\n\n\nMy work here was to show the variation of median age within New Zealand but also to explore whether or not New Zealand has a young or ageing-population and explore that question in it’s specific regions as well.\nIt is clear that the West Coast has the oldest median age population at 48.1 as it is colored dark blue, why is that? And on the other hand what makes it so that Auckland has a median age more than 10 years younger than West Coast at 35.9 showing up in light yellow. How can regions be so close but vary so largely in median age. What are the attractions in each area that would make it more suitable for a specific age group?",
    "crumbs": [
      "Best Work",
      "Solo Project"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html",
    "href": "ica/quarto-demo.html",
    "title": "My first Quarto document",
    "section": "",
    "text": "Intro\nMacalester College is in the Twin Cities. It has:\nCheck it out for yourself:",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#intro",
    "href": "ica/quarto-demo.html#intro",
    "title": "My first Quarto document",
    "section": "",
    "text": "four seasons\nbagpipes\ndelightful students",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "href": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "title": "My first Quarto document",
    "section": "Exercise 1: Deduce Quarto features",
    "text": "Exercise 1: Deduce Quarto features\nCheck out the appearance and contents of this document. Thoughts?\nIn the toolbar at the top of this document, Render the .qmd file into a .html file. Where is this file stored? Thoughts about its appearance / contents? Can you edit it?\nToggling between the .qmd and .html files, explain the purpose of the following features in the .qmd file:\n*\n**\n#\n-\n\\\n![](url)",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-2-code",
    "href": "ica/quarto-demo.html#exercise-2-code",
    "title": "My first Quarto document",
    "section": "Exercise 2: Code",
    "text": "Exercise 2: Code\nHow does this appear in the .qmd? The .html? So…?!\nseq(from = 100, to = 1000, by = 50)",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-3-chunks",
    "href": "ica/quarto-demo.html#exercise-3-chunks",
    "title": "My first Quarto document",
    "section": "Exercise 3: Chunks",
    "text": "Exercise 3: Chunks\nQuarto isn’t a mind reader – we must distinguish R code from text. We do so by putting code inside an R chunk:\n\nPut the seq() code in the chunk.\nPress the green arrow in the top right of the chunk. What happens in the qmd?\nRender. What appears in the html: R code, output, or both?",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-4-practice",
    "href": "ica/quarto-demo.html#exercise-4-practice",
    "title": "My first Quarto document",
    "section": "Exercise 4: Practice",
    "text": "Exercise 4: Practice\n\nUse R code to create the following sequence: 10 10 10 10\nStore the sequence as four_tens.\nUse an R function (which we haven’t learned!) to add up the numbers in four_tens.",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "href": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "title": "My first Quarto document",
    "section": "Exercise 5: Fix this code",
    "text": "Exercise 5: Fix this code\nCode is a form of communication, and the code below doesn’t cut it.\nPut the code in a chunk and fix it.\nRep(x = 1, times = 10) seq(from=100,to=1000,length=20) theNumberofStudentsinthisclass&lt;-27",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-6-comments",
    "href": "ica/quarto-demo.html#exercise-6-comments",
    "title": "My first Quarto document",
    "section": "Exercise 6: Comments",
    "text": "Exercise 6: Comments\nRun the chunk below. Notice that R ignores anything in a line starting with a pound sign (#). If we took the # away we’d get an error!\n\nCode# This is a comment\n4 + 5\n\n[1] 9\n\n\nWe’ll utilize this feature to comment our code, i.e. leave short notes about what our code is doing. Below, replace the ??? with an appropriate comment.\n\nCode# ???\ntemperature_c &lt;- 10\ntemperature_f &lt;- temperature_c * 9/5 + 32\ntemperature_f\n\n[1] 50",
    "crumbs": [
      "In-class Activities",
      "My first Quarto document"
    ]
  },
  {
    "objectID": "ica/ica-uni.html",
    "href": "ica/ica-uni.html",
    "title": "Univariate Viz",
    "section": "",
    "text": "Exercises\nCode# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")",
    "crumbs": [
      "In-class Activities",
      "Univariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#exercises",
    "href": "ica/ica-uni.html#exercises",
    "title": "Univariate Viz",
    "section": "",
    "text": "Exercise 1: Research Questions\nLet’s dig into the hikes data, starting with the elevation and difficulty ratings of the hikes:\n\nCodehead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture?\nWhat about a visualization of the quantitative elevation variable?\nExercise 2: Load tidyverse\nWe’ll address the above questions using ggplot tools. Try running the following chunk and simply take note of the error message – this is one you’ll get a lot!\n\nCodelibrary(tidyverse)\n# Use the ggplot function\nggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\nIn order to use ggplot tools, we have to first load the tidyverse package in which they live. We’ve installed the package but we need to tell R when we want to use it. Run the chunk below to load the library. You’ll need to do this within any .qmd file that uses ggplot().\n\nCode# Load the package\nlibrary(tidyverse)\n\n\nExercise 3: Bar Chart of Ratings - Part 1\nConsider some specific research questions about the difficulty rating of the hikes:\n\nHow many hikes fall into each category?\nAre the hikes evenly distributed among these categories, or are some more common than others?\n\nAll of these questions can be answered with: (1) a bar chart; of (2) the categorical data recorded in the rating column. First, set up the plotting frame:\n\nCodeggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\nThink about:\n\nWhat did this do? What do you observe?\nWhat, in general, is the first argument of the ggplot() function?\nWhat is the purpose of writing x = rating?\nWhat do you think aes stands for?!?\nExercise 4: Bar Chart of Ratings - Part 2\nNow let’s add a geometric layer to the frame / canvas, and start customizing the plot’s theme. To this end, try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\nNOTE:\n\nPay attention to the general code properties and structure, not memorization.\nNot all of these are “good” plots. We’re just exploring ggplot.\n\n\nCode# added a geom_bar() which created a bar graph visual on top of the frame\n\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nCode# added labels that are more descriptive to compare two different categories\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# Color coded the bar fillings to be blue making it stand out more\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# outlined the bar chunks with orange \nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# added \" + theme_minimal()\" to the code which appears to make the background theme white and more basic \nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 5: Bar Chart Follow-up\nPart a\nReflect on the ggplot() code.\n\nWhat’s the purpose of the +? When do we use it? when we are adding layers to the viz\nWe added the bars using geom_bar()? Why “geom”? geom because the bars are geometric objects to relay this data\nWhat does labs() stand for? labels\nWhat’s the difference between color and fill? fill colors in the entire bar and color is the outline of the bar. #### Part b {.unnumbered}\n\nIn general, bar charts allow us to examine the following properties of a categorical variable:\n\n\nobserved categories: What categories did we observe?\n\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others?\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\nWe learned from the bar chart that many hikes are moderate and the fewest hikes are difficult.\nPart c\nIs there anything you don’t like about this barplot? For example: check out the x-axis again.\nI dont like how it just says rating instead of difficulty rating and I wish it went easy, moderate, difficult\nExercise 6: Sad Bar Chart\nLet’s now consider some research questions related to the quantitative elevation variable:\n\nAmong the hikes, what’s the range of elevation and how are the hikes distributed within this range (e.g. evenly, in clumps, “normally”)? 3820-5344 distributed relatively normal\nWhat’s a typical elevation?\nAre there any outliers, i.e. hikes that have unusually high or low elevations?\n\nHere:\n\nConstruct a bar chart of the quantitative elevation variable.\nExplain why this might not be an effective visualization for this and other quantitative variables. (What questions does / doesn’t it help answer?)\n\nthe elevations vary A LOT so it is hard visualize it nicely.\n\nCodeggplot(hikes, aes(x = elevation)) + \n  geom_bar()\n\n\n\n\n\n\n\nExercise 7: A Histogram of Elevation\nQuantitative variables require different viz than categorical variables. Especially when there are many possible outcomes of the quantitative variable. It’s typically insufficient to simply count up the number of times we’ve observed a particular outcome as the bar graph did above. It gives us a sense of ranges and typical outcomes, but not a good sense of how the observations are distributed across this range. We’ll explore two methods for graphing quantitative variables: histograms and density plots.\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Check out the example below:\n\nPart a\nLet’s dig into some details.\n\nHow many hikes have an elevation between 4500 and 4700 feet? 6 hikes\nHow many total hikes have an elevation of at least 5100 feet? 2 hikes\nPart b\nNow the bigger picture. In general, histograms allow us to examine the following properties of a quantitative variable:\n\n\ntypical outcome: Where’s the center of the data points? What’s typical? 4400 (hike #23)\n\nvariability & range: How spread out are the outcomes? What are the max and min outcomes? 5500-3700 ft\n\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)? relatively symmetric\n\noutliers: Are there any outliers, i.e. outcomes that are unusually large/small? not that really stand out\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Addressing each of the features in the above list, summarize below what you learned from the histogram, in context.\nExercise 8: Building Histograms - Part 1\n2-MINUTE CHALLENGE: Thinking of the bar chart code, try to intuit what line you can tack on to the below frame of elevation to add a histogram layer. Don’t forget a +. If it doesn’t come to you within 2 minutes, no problem – all will be revealed in the next exercise.\n\nCodelibrary(ggplot2)\nggplot(hikes, aes(x = elevation))\n\n\n\n\n\n\n\nExercise 9: Building Histograms - Part 2\nLet’s build some histograms. Try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\n\nCode# added the code geom_histogram() which layered a graph of the data (specifically a histogram visual) onto the frame\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nCode# outlined the histograms bars in white \nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") \n\n\n\n\n\n\n\n\nCode# filled the data bars in with blue \nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") \n\n\n\n\n\n\n\n\nCode# created good labels for the x and y axis \nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# changed the width of the bin that each hike falls into to \nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nCode# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 10: Histogram Follow-up\n\nWhat function added the histogram layer / geometry?\nWhat’s the difference between color and fill?\nWhy does adding color = \"white\" improve the visualization?\nWhat did binwidth do?\nWhy does the histogram become ineffective if the binwidth is too big (e.g. 1000 feet)?\nWhy does the histogram become ineffective if the binwidth is too small (e.g. 5 feet)?\nExercise 11: Density Plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting observations into discrete bins, the “density” of observations is calculated across the entire range of outcomes. The greater the number of observations, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\nCheck out a density plot of elevation. Notice that the y-axis (density) has no contextual interpretation – it’s a relative measure. The higher the density, the more common are elevations in that range.\n\nCodeggplot(hikes, aes(x = elevation)) +\n  geom_density()\n\n\n\n\n\n\n\nQuestions\n\n\nINTUITION CHECK: Before tweaking the code and thinking back to geom_bar() and geom_histogram(), how do you anticipate the following code will change the plot?\n\ngeom_density(color = \"blue\")\ngeom_density(fill = \"orange\")\n\n\nTRY IT! Test out those lines in the chunk below. Was your intuition correct?\n\n\nExamine the density plot. How does it compare to the histogram? What does it tell you about the typical elevation, variability / range in elevations, and shape of the distribution of elevations within this range?\nExercise 12: Density Plots vs Histograms\nThe histogram and density plot both allow us to visualize the behavior of a quantitative variable: typical outcome, variability / range, shape, and outliers. What are the pros/cons of each? What do you like/not like about each?\nExercise 13: Code = communication\nWe obviously won’t be done until we talk about communication. All code above has a similar general structure (where the details can change):\n\nCodeggplot(___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\n\nThough not necessary to the code working, it’s common, good practice to indent or tab the lines of code after the first line (counterexample below). Why?\n\n\nCode# YUCK\nggplot(hikes, aes(x = elevation)) +\ngeom_histogram(color = \"white\", binwidth = 200) +\nlabs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nThough not necessary to the code working, it’s common, good practice to put a line break after each + (counterexample below). Why?\n\n\nCode# YUCK \nggplot(hikes, aes(x = elevation)) + geom_histogram(color = \"white\", binwidth = 200) + labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 14: Practice\nPart a\nPractice your viz skills to learn about some of the variables in one of the following datasets from the previous class:\n\nCode# Data on students in this class\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\n\nPart b\nCheck out the RStudio Data Visualization cheat sheet to learn more features of ggplot.\n\n\n\n\n\n\nCheck → Commit → Push\n\n\n\nWhen done, don’t forgot to click Render Book and check the resulting HTML files. If happy, jump to GitHub Desktop and commit the changes with the message Finish activity 3 and push to GitHub. Wait few seconds, then visit your portfolio website and make sure the changes are there.",
    "crumbs": [
      "In-class Activities",
      "Univariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#solutions",
    "href": "ica/ica-uni.html#solutions",
    "title": "Univariate Viz",
    "section": "Solutions",
    "text": "Solutions\n\nClick for Solutions\nExercise 1: Research Questions\n\nFor example: how many hikes are there in each category? are any categories more common than others?\nFor example: What’s a typical elevation? What’s the range in elevations?\nExercise 3: Bar Chart of Ratings - Part 1\n\nCodeggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\n\njust a blank canvas\nname of the dataset\nindicate which variable to plot on x-axis\n\naesthetics\nExercise 4: Bar Chart of Ratings - Part 2\n\nCode# Add a bar plot LAYER\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n\nCode# Add meaningful axis labels\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# FILL the bars with blue\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# COLOR the outline of the bars in orange\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# Change the theme to a white background\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") + \n  theme_minimal()\n\n\n\n\n\n\n\nExercise 5: Bar Chart Follow-up\nPart a\n\nTo indicate we’re still adding layers to / modifying our plot.\nBars are the geometric elements we’re adding in this layer.\nlabels\n\nfill fills in the bars. color outlines the bars.\nPart b\nMost hikes are moderate, the fewest number are difficult.\nPart c\nI don’t like that the categories are alphabetical, not in order of difficulty level.\nExercise 6: Sad Bar Chart\nThere are too many different outcomes of elevation.\n\nCodeggplot(hikes, aes(x = elevation)) + \n  geom_bar()\n\n\n\n\n\n\n\nExercise 7: A Histogram of Elevation\nPart a\n\n6\n1 + 1 = 2\nPart b\nElevations range from roughly 3700 to 5500 feet. Elevations vary from hike to hike relatively normally (with a bell shape) around a typical elevation of roughly 4500 feet.\nExercise 9: Building Histograms - Part 2\n\nCodelibrary(ggplot2)\n\n#| eval: true\n\n# Add a histogram layer\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n\n\n\n\n\nCode# Outline the bars in white\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") \n\n\n\n\n\n\nCode# Fill the bars in blue\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") \n\n\n\n\n\n\nCode# Add axis labels\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# Change the width of the bins to 1000 feet\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# Change the width of the bins to 5 feet\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\nCode# Change the width of the bins to 200 feet\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 10: Histogram Follow-up\n\ngeom_histogram()\n\ncolor outlined the bars and fill filled them\neasier to distinguish between the bars\nchanged the bin width\nwe lump too many hikes together and lose track of the nuances\nwe don’t lump enough hikes together and lose track of the bigger picture trends\nExercise 11: Density plots\n\nCodeggplot(hikes, aes(x = elevation)) +\n geom_density(color = \"blue\", fill = \"orange\")\n\n\n\n\n\n\n\nExercise 13: Code = Communication\n\nClarifies that the subsequent lines are a continuation of the first. That is, we’re not done with the plot yet. These lines are all part of the same idea.\nThis is like a run-on sentence. It’s tough to track the distinct steps that go into building the plot.",
    "crumbs": [
      "In-class Activities",
      "Univariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "Bivariate Viz",
    "section": "",
    "text": "Use this file for practice with the bivariate viz in-class activity. Refer to the class website for details.\n\nCode# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\n\nCodelibrary(ggplot2)\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone? it has two variables from the data instead of just one\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n\n\n\n\n\n\nCode# Add a layer of points for each county(\"geom_point()\")\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n\n\n\n\n\n\nCode# Change the shape of the points\n# What happens if you change the shape to another number? the data points change their shape \nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 3)\n\n\n\n\n\n\n\n\nCode# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"ORANGE\" )\n\n\n\n\n\n\n\n\nCode# Add a layer that represents each county by the state it's in\n# Take note of the geom and the info it needs to run!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_text(aes(label = state_abbr)) \n\n\n\n\n\n\n\n#repub pct 20 and repub pct 16 have a strong positive relationship with a few possible outliers in texas.\n\nCodeggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nCodeggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\nCodeggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nCode# Scatterplot of repub_pct_20 vs median_rent\n\nggplot(elections, aes(y = repub_pct_20, x = median_rent)) +\n  geom_point(color = \"red\") + geom_smooth()\n\n\n\n\n\n\nCode# they have a moderately weak negative relationship THIS ONE IS BETTER AT PREDICTING\n\n\n# Scatterplot of repub_pct_20 vs median_age\n\nggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point(color = \"light blue\" ) + geom_smooth()\n\n\n\n\n\n\nCode# they have a weak positive relationship \n\n\n\nCodeggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\n\n\nCode# not that effective of a visual because it doesn't show the numerical trends/ and shaope of the data to determine a relationship\n\n\n\nCode# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nCode# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe 2020 republican country-level support had many red and purple voter outliers below 50 but also had high median levels of support above 50 for each color.\n\nCodeggplot(elections, aes(x = repub_pct_20)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCode# Name two \"bad\" things about this plot\n# 1) the colors do not match which can be misleading\n# 2) the density are on top of eachother \nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCode# What does scale_fill_manual do?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))",
    "crumbs": [
      "In-class Activities",
      "Bivariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-multi.html",
    "href": "ica/ica-multi.html",
    "title": "Multivariate Viz",
    "section": "",
    "text": "Exercises (required)",
    "crumbs": [
      "In-class Activities",
      "Multivariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-required",
    "href": "ica/ica-multi.html#exercises-required",
    "title": "Multivariate Viz",
    "section": "",
    "text": "The story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\nCode# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\nExercise 1: SAT scores\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nCodelibrary(tidyverse)\n\nggplot(education, aes(x=sat))+\n  geom_density()\n\n\n\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?)\nthe SAT scores range from about 840 - 1200 the shape of the plot is slightly skewed right possibly this shape is because there are averages in two specific spots rather than incremental increases in scores. (people either do bad, good, or fantastic)\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\nPart a\n\nCode# Construct a plot of sat vs expend\nggplot(education, aes(y=sat,x=expend))+geom_point()+geom_smooth(method = \"lm\")+ geom_point()\n\n\n\n\n\n\nCode# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\n\n\nCode# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\nggplot(education,aes(y=sat,x=salary))+geom_smooth(method=\"lm\")+geom_point()\n\n\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you?\nI’m surpised that the on the right-hand side where salary is higher the SAT score drop/ are lower than the middle region where teachers are paid less. I would assume the more money teachers are getting the more drive they have to prepare their students for the SAT.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nCodeggplot(education,aes(y=sat,x=salary,color=expend))+geom_smooth(method=\"lm\")+geom_point()\n\n\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\nbreaks it up into 3 cuts rather than 2 and so on (CREATES RANGES)\n\nCodeggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n\nDescribe the trivariate relationship between sat, salary, and expend.\nas expend and salary increase the SAT scores decrease.\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\nCodeggplot(education, aes(x = fracCat))+geom_bar()\n\n\n\n\n\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\nMore students taking it = lower scores (probably because where there are less students are the ones who are confident and study more)\n\nCodeggplot(education,aes(fill=fracCat,x=sat))+geom_density(alpha=0.5)\n\n\n\n\n\n\nCodeggplot(education,aes(x=fracCat,y=sat))+geom_point()\n\n\n\n\n\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\nno in this case SAT scores are increasing with spending\n\nCodeggplot(education, aes(y = sat, x = expend, color = fracCat)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?\nIt appeared that way because the less money spent on spending the less kids who think they have a chance to do well so less participation so only the best kids participate increasing the low expend areas score values.",
    "crumbs": [
      "In-class Activities",
      "Multivariate Viz"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html",
    "href": "ica/ica-spatial.html",
    "title": "Spatial Viz",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "In-class Activities",
      "Spatial Viz"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#exercises",
    "href": "ica/ica-spatial.html#exercises",
    "title": "Spatial Viz",
    "section": "",
    "text": "Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\nPart 1: Interactive points on a map with leaflet\n\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\nCodefave_places &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\n  latitude longitude\n1       59        18\n2       45       -93\n3       33      -117\n4       40       116\n5       40       106\n6       37      -122\n\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\nCode# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\nleaflet(data = fave_places)\n\n\n\n\n\n\nCode# Now what do we have?\nleaflet(data = fave_places) |&gt; \n  addTiles()\n\n\n\n\n\n\nCode# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\nCode# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into? How does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\nCode# Load package needed to change color\nlibrary(gplots)\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\nCode# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\n\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\n\nweight = how thick to make the lines, points, pixels\n\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\nCode# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\n\nLet’s focus on only those in Minnesota for now:\n\nCode# Don't worry about the syntax\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(leaflet)\nstarbucks_mn &lt;- starbucks |&gt;   \n  filter(Country == \"US\", State.Province == \"MN\")\n\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\n\nCodeleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(color =\"red\", lng = ~Longitude, lat = ~Latitude)\n\n\n\n\n\nPart 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can not print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\n\nCodeggplot(starbucks, aes(y = Latitude, x = Longitude)) + \n  geom_point(size = 0.5)\n\n\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\nCode# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\nPart b\nRun the chunks below to build up a new map.\n\nCode# What does this code produce?\n#an outline map of the world \n# What geom are we using for the point map?\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\nCode# Load package needed to change map theme\nlibrary(mosaic)\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?!\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map.",
    "crumbs": [
      "In-class Activities",
      "Spatial Viz"
    ]
  },
  {
    "objectID": "ica/Effectiveica.html",
    "href": "ica/Effectiveica.html",
    "title": "Effective Viz",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "In-class Activities",
      "Effective Viz"
    ]
  },
  {
    "objectID": "ica/Effectiveica.html#exercises",
    "href": "ica/Effectiveica.html#exercises",
    "title": "Effective Viz",
    "section": "",
    "text": "Exercise 1: Professionalism\nLet’s examine weather in 3 Australian locations.\n\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n# Import the data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))\n\nThe following plot is fine for things like homework or just playing around. But we’ll make it more “professional” looking below.\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\n\n\n\n\n\n\nPart a\nReplace A, B, C, and D in the code below to:\n\nAdd a short, but descriptive title. Under 10 words.\nChange the x- and y-axis labels, currently just the names of the variables in the dataset. These should be short and include units.\nChange the legend title to “Location” (just for practice, not because it’s better than “location”).\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"A\", y = \"B\", title = \"C\", color = \"D\")  \n\n\n\n\n\n\n\nPart b\nWhen we’re including our plot in an article, paper, book, or other similar outlet, we should (and are expected to) provide a more descriptive figure caption. Typically, this is instead of a title and is more descriptive of what exactly is being plotted.\n\nAdd a figure caption in the top of the chunk.\nInclude your x-axis, y-axis, and legend labels from Part a.\nRender your Rmd and check out how the figure caption appears.\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"???\", y = \"???\", color = \"???\")  \n\n\n\n???\n\n\n\n\n\n\n\n???\n\n\n\n\nExercise 2: Accessibility\nLet’s now make a graphic more accessible.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\nPart a\nLet’s add some alt text that can be picked up by screen readers. This is a great resource on writing alt text for data viz. In short, whereas figure captions are quick descriptions which assume that the viz is accessible, alt text is a longer description which assumes the viz is not accessible. Alt text should concisely articulate:\n\nWhat your visualization is (e.g. a density plot of 3pm temperatures in Hobart, Uluru, and Wollongong, Australia).\nA 1-sentence description of the most important takeaway.\nA link to your data source if it’s not already in the caption.\n\nAdd appropriate alt text at the top of the chunk, in fig-alt. Then render your qmd, and hover over the image in your rendered html file to check out the alt text.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\nPart b\nColor is another important accessibility consideration. Let’s check out the color accessibility of our density plot.\n\nRun the ggplot() code from Part a in your console. The viz will pop up in the Plots tab.\nIn the Plots tab, click “Export” then “Save as image”. Save the image somewhere.\nNavigate to https://www.color-blindness.com/coblis-color-blindness-simulator/\n\nAbove the image of crayons (I think it’s crayons?), click “Choose file” and choose the plot file you just saved.\nClick the various simulator buttons (eg: Red-Weak/Protanomaly) to check out how the colors in this plot might appear to others.\nSummarize what you learn. What impact might our color choices have on one’s ability to interpret the viz?\nPart c\nWe can change our color schemes! There are many color-blind friendly palettes in R. In the future, we’ll set a default, more color-blind friendly color theme at the top of our Rmds. We can also do this individually for any plot that uses color. Run the chunks below to explore various options.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_fill_viridis_d()    \n\n\n\n\n\n\n\n\n# In the color scale line:\n# Change \"fill\" to \"color\" since we use color in the aes()\n# Change \"d\" (discrete) to \"c\" (continuous) since maxtemp is on a continuous scale\nggplot(weather, aes(y = temp3pm, x = temp9am, color = maxtemp)) + \n  geom_point(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nExercise 3: Ethics\nLet’s scratch the surface of ethics in data viz. Central to this discussion is the consideration of impact.\nPart a\nAt a minimum, our data viz should not mislead. Reconsider the climate change example from above. Why is this plot unethical and what impact might it have on policy, public opinion, etc?\n\nPart b\nAgain, data viz ethical considerations go beyond whether or not a plot is misleading. As described in the warm-up, we need to consider: visibility, privacy, power, emotion & embodiment, pluralism, & context. Depending upon the audience and goals of a data viz, addressing these points might require more nuance. Mainly, the viz tools we’ve learned are a great base or foundation, but aren’t the only approaches to data viz. \nPick one or more of the following examples of data viz to discuss with your group. How do the approaches taken:\n\nemphasize one or more of: visibility, privacy, power, emotion, embodiment, pluralism, and/or context?\nimprove upon what we might be able to convey with a simpler bar chart, scatterplot, etc?\n\n\nExample: W.E.B. Du Bois (1868–1963)\nDu Bois (“Doo Boys”) was a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1. He was also a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. Du Bois noted: “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. NOTE: This work uses language common to that time period and addresses the topic of slavery. Check out:\n\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\nAn article by Allen Hillery (@AlDatavizguy).\n\n\nExample: One person’s experience with long COVID\nNYT article\n\nExample: Decolonizing data viz\nblog post\n\nExample: Visualizing climate change through art\nFutures North with Prof John Kim & Mac students (by Prof Kim, Mac research students)\n\nExample: Personal data collection\nDear Data\n\nPart c\nFor a deeper treatment of similar topics, and more examples, read Data Feminism.\n\nExercise 4: Critique\nPractice critiquing some more complicated data viz listed at Modern Data Science with R, Exercise 2.5.\nThink about the following questions:\n\nWhat story does the data graphic tell? What is the main message that you take away from it?\nCan the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nCritique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Are there things that you would have done differently?\n\n\nExercise 5: Design Details\nThis final exercise is just “food for thought”. It’s more of a discussion than an exercise, and gets into some of the finer design details and data viz theory. Go as deep or not deep as you’d like here.\nIn refining the details of our data viz, Visualize This and Storytelling with Data provide some of their guiding principles. But again, every context is different.\n\nPut yourself in a reader’s shoes. What parts of the data need explanation?\nShine a light on your data. Try to remove any “chart junk” that distracts from the data.\nVary color and style to emphasize the viz elements that are most important to the story you’re telling.\nIt is easier to judge length than it is to judge area or angles.\nBe thoughtful about how your categories are ordered for categorical data.\n\nGetting into even more of the nitty gritty, we need to be mindful of what geometric elements and aesthetics we use. The following elements/aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor. (Color is the most difficult, because it is a 3-dimensional quantity.)\n\nFinally, here are some facts to keep in mind about visual perception from Now You See It.\nPart a: Selectivity\nVisual perception is selective, and our attention is often drawn to contrasts from the norm.\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\nExample: What stands out in this example image? This is originally from C. Ware, Information Visualization: Perception for Design, 2004? Source: S. Few, Now You See It, 2009, p. 33.\n\nPart b: Familiarity\nOur eyes are drawn to familiar patterns. We observe what we know and expect.\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\nExample: Do you notice anything embedded in this rose image from coolbubble.com? Source: S. Few, Now You See It, 2009, p. 34.\n\nPart c: Revisit\nRevisit Part b. Do you notice anything in the shadows? Go to https://mac-stat.github.io/images/112/rose2.png for an image.\n\nWrapping up\nIf you finish early:\n\nWork on homework if not done already\nComplete any activities you haven’t finished yet, eg, spatial viz, the optional but fun exercises in the Multivariate viz and Bivariate viz activities.\nIf you’ve done all that, explore some datasets in TidyTuesday.",
    "crumbs": [
      "In-class Activities",
      "Effective Viz"
    ]
  },
  {
    "objectID": "ica/Effectiveica.html#solutions",
    "href": "ica/Effectiveica.html#solutions",
    "title": "Effective Viz",
    "section": "Solutions",
    "text": "Solutions\nThe exercises today are discussion based. There are no “solutions”. Happy to chat in office hours about any ideas here!",
    "crumbs": [
      "In-class Activities",
      "Effective Viz"
    ]
  },
  {
    "objectID": "ica/Effectiveica.html#footnotes",
    "href": "ica/Effectiveica.html#footnotes",
    "title": "Effective Viz",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎\nB. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15.↩︎",
    "crumbs": [
      "In-class Activities",
      "Effective Viz"
    ]
  },
  {
    "objectID": "ica/wranglingICA.html",
    "href": "ica/wranglingICA.html",
    "title": "Wrangling",
    "section": "",
    "text": "Code# Define elections_small\n\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nelections_small &lt;- elections |&gt;\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\n\nCode# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\nCode# What does this do? filters values of state names only of Hawaii and Delaware \nelections_small |&gt;\n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\nCode# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small |&gt;\nfilter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\nCode# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row (observation) than your answer above\nelections_small |&gt;\nfilter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\n\nCode# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\n# elections_small |&gt;\n#  filter(___) |&gt;\n#  filter(___)\n\n# Method 2: 1 filter with 2 conditions\n#elections_small |&gt; \n#  filter(___, ___)",
    "crumbs": [
      "In-class Activities",
      "Wrangling"
    ]
  },
  {
    "objectID": "ica/datesICA.html",
    "href": "ica/datesICA.html",
    "title": "Dates",
    "section": "",
    "text": "Codelibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\nCode# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species %in% c(\"Adelie\",\"Chinstrap\")) |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\nCode# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\nCode# NOTE the use of is.na()\npenguins |&gt; \n  summarize(sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  `sum(is.na(body_mass_g))`\n                      &lt;int&gt;\n1                         2\n\n\n\nCode# NOTE the use of is.na()\npenguins_w_body_mass &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g))\n\n# Compare the number of penguins in this vs the original data\nnrow(penguins_w_body_mass)\n\n[1] 342\n\n\n\nCodenrow(penguins)\n\n[1] 344\n\n\n\nCodepenguins_w_body_mass |&gt; \n  summarize(sum(is.na(sex)))\n\n# A tibble: 1 × 1\n  `sum(is.na(sex))`\n              &lt;int&gt;\n1                 9\n\n\n\nCodepenguins_complete &lt;- penguins |&gt; \n  na.omit()\n\n\n\nCodenrow(penguins_complete)\n\n[1] 333\n\n\n\nCodenrow(penguins)\n\n[1] 344\n\n\n\nCode# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\nCode# Use a shortcut to keep everything but the year and island variables\npenguins |&gt;\nselect(-year,-island) \n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\nCode# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; \n  select(species,ends_with(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\nCode# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt; \n  select(species, starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\nCode# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt; \n  select(species,contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\n\nCode# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\nCode# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  count(species,sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\nCode# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\n\nCode# Get today's date\nas.Date(today())\n\n[1] \"2025-05-01\"\n\n\n\nCode# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\n\nCodeyear(today)\n\n[1] 2025\n\n\n\nCode# What do these lines produce / what's their difference?\nmonth(today)\n\n[1] 5",
    "crumbs": [
      "In-class Activities",
      "Dates"
    ]
  },
  {
    "objectID": "ica/ICAreshapingg.html",
    "href": "ica/ICAreshapingg.html",
    "title": "Reshaping",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nCodesleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86\n\n\nunits of observation are the subjects.\nI would need to create a column that holds all the reaction times and then a day column\n#ggplot(sleep_wide, aes(y = rxn time, x = day, color = Subject))\n\nCodesleep_day_rxn &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", names_prefix = \"day_\", values_to = \"rxn_time\")\n\nggplot(sleep_day_rxn, aes(y=rxn_time,x=day, color =Subject))+\n  geom_line()\n\n\n\n\n\n\n\n\nCodesleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt;\n  mutate(Subject = as.factor(Subject),day = as.numeric(day))\nhead(sleep_long,3)\n\n# A tibble: 3 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n\n\n#changed also to make each of the 9 days of the subject study broken up\n\nCodeggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\n\nCode# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(y = reaction_time, x = day)) + \n  geom_line() + \n  facet_wrap(~ Subject)\n\n\n\n\n\n\n\nreaction time gets much worse around day 5 of lack of sleep for the majority of subjects, meaning less sleep results in a slower reaction time.\n\nCode# sleep_long |&gt;\n#   pivot_wider(names_from = , values_from = ___) |&gt;\n#   head()",
    "crumbs": [
      "In-class Activities",
      "Reshaping"
    ]
  },
  {
    "objectID": "ica/joining.html",
    "href": "ica/joining.html",
    "title": "Joining",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "In-class Activities",
      "Joining"
    ]
  },
  {
    "objectID": "ica/joining.html#exercises",
    "href": "ica/joining.html#exercises",
    "title": "Joining",
    "section": "",
    "text": "Exercise 1: Where are my keys?\nPart a\nDefine two new datasets, with different students and courses:\n\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\nstudents_1 &lt;- data.frame(\n  student = c(\"A\", \"B\", \"C\"),\n  class = c(\"STAT 101\", \"GEOL 101\", \"ANTH 101\")\n)\n\n# Check it out\nstudents_1\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n3       C ANTH 101\n\n\n\nenrollments_1 &lt;- data.frame(\n  class = c(\"STAT 101\", \"ART 101\", \"GEOL 101\"),\n  enrollment = c(18, 17, 24)\n)\n\n# Check it out\nenrollments_1\n\n     class enrollment\n1 STAT 101         18\n2  ART 101         17\n3 GEOL 101         24\n\n\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\nstudents_2 |&gt; \n  left_join(enrollments_2)\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(class == course))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# join_by(\"left data key\" == \"right data key\")\n# The order is mixed up here, thus we get an error:\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(course == class))\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable as a key. What are grade.x and grade.y?\n\nstudents_3 |&gt; \n  left_join(enrollments_3, join_by(class == class))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\n\n\n# 2. We want contact info for people who HAVE voted\n\n\n# 3. We want any data available on each person\n\n\n# 4. When possible, we want to add contact info to the voting roster\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\n\nsid = student ID\n\ngrade = student’s grade\n\nsessionID = an identifier of the class section\n\n\n# Get rid of some duplicate rows!\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n  distinct(sid, sessionID, .keep_all = TRUE)\nhead(grades)\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\n\nsessionID = an identifier of the class section\n\ndept = department\n\nlevel = course level (eg: 100)\n\nsem = semester\n\nenroll = enrollment (number of students)\n\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\n\n\n# How many observations (rows) and variables (columns) are there in the courses data?\n\nExercise 4: Class size\nHow big are the classes?\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\n    sessionID dept level    sem enroll     iid\n1 session2047    g   100 FA2001     12 inst436\n2 session2047    m   100 FA2001     28 inst436\n\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\n# courses_combined &lt;- courses |&gt; \n#   ___(sessionID) |&gt; \n#   ___(enroll = sum(___))\n\n# Check that this has 1695 rows and 2 columns\n# dim(courses_combined)\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\n\n# ggplot(student_class_size, aes(x = ___)) + \n#   geom___()\n\nExercise 5: Narrowing in on classes\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\nPart b\nWhat’s the grade-point average (GPA) for each student?\nPart c\nWhat’s the median GPA across all students?\nPart d\nWhat fraction of grades are below B+?\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins.",
    "crumbs": [
      "In-class Activities",
      "Joining"
    ]
  },
  {
    "objectID": "ica/joining.html#solutions",
    "href": "ica/joining.html#solutions",
    "title": "Joining",
    "section": "Solutions",
    "text": "Solutions\n\nClick for Solutions\nExample 1\n\nclass\na student that took ANTH 101\ndata on ART 101\nExample 2\n\nWhat did this do? Linked course info to all students in students_1\n\nWhich observations from students_1 (the left table) were retained? All of them.\nWhich observations from enrollments_1 (the right table) were retained? Only STAT and GEOL, those that matched the students.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. We retain the courses, not students.\n\n\nenrollments_1 |&gt; \n  left_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n\n\nExample 3\n\nWhich observations from students_1 (the left table) were retained? A and B, only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? STAT and GEOL, only those with studen info.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same info, different column order.\n\n\nenrollments_1 |&gt; \n    inner_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2 GEOL 101         24       B\n\n\nExample 4\n\nWhich observations from students_1 (the left table) were retained? All\nWhich observations from enrollments_1 (the right table) were retained? All\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n    full_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n4 ANTH 101         NA       C\n\n\nExample 5\n\nWhich observations from students_1 (the left table) were retained? Only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n  semi_join(students_1)\n\n     class enrollment\n1 STAT 101         18\n2 GEOL 101         24\n\n\nExample 6\n\nWhich observations from students_1 (the left table) were retained? Only C, the one without enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Retain only ART 101, the course with no student info.\n\n\nenrollments_1 |&gt; \n  anti_join(students_1)\n\n    class enrollment\n1 ART 101         17\n\n\nExercise 2: More small practice\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt; \n  anti_join(voters, join_by(name == id))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt; \n  semi_join(voters, join_by(name == id))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. We want any data available on each person\ncontact |&gt; \n  full_join(voters, join_by(name == id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\nvoters |&gt; \n  full_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n6  B          NA    grand  89\n7  C          NA snelling  43\n\n# 4. We want to add contact info, when possible, to the voting roster\nvoters |&gt; \n  left_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\nExercise 3: Bigger datasets\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\nExercise 4: Class size\nPart a\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\n\ncourses_combined |&gt; \n  summarize(median(enroll))\n\nPart c\n\nstudent_class_size &lt;- grades |&gt; \n  left_join(courses_combined) |&gt; \n  group_by(sid) |&gt; \n  summarize(med_class = median(enroll))\n\nhead(student_class_size)\n\nPart d\n\nggplot(student_class_size, aes(x = med_class)) +\n  geom_histogram(color = \"white\")\n\nExercise 5: Narrowing in on classes\nPart a\n\ngrades |&gt; \n  filter(sessionID == \"session1986\")\n\nPart b\n\ngrades |&gt; \n  semi_join(dept_E)\n\nExercise 6: All the wrangling\nPart a\n\ncourses |&gt; \n  group_by(dept) |&gt; \n  summarize(total = sum(enroll)) |&gt; \n  arrange(desc(total))\n\nPart b\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(mean(gp, na.rm = TRUE))\n\nPart c\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  summarize(median(gpa))\n\nPart d\n\n# There are lots of approaches here!\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  mutate(below_b_plus = (gp &lt; 3.3)) |&gt; \n  summarize(mean(below_b_plus, na.rm = TRUE))\n\nPart e\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  left_join(courses) |&gt; \n  group_by(iid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\nPart f\n\ncross_listed &lt;- courses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\ngrades |&gt; \n  anti_join(cross_listed) |&gt; \n  inner_join(courses) |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(dept) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)",
    "crumbs": [
      "In-class Activities",
      "Joining"
    ]
  },
  {
    "objectID": "ica/Factors.html",
    "href": "ica/Factors.html",
    "title": "Factors",
    "section": "",
    "text": "Exercises\nThe exercises revisit our grades data:\nCodelibrary(tidyverse)\n\n\n# Get rid of some duplicate rows!\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n  distinct(sid, sessionID, .keep_all = TRUE)\n\n# Check it out\nhead(grades)\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\nWe’ll explore the number of times each grade was assigned:\nCodegrade_distribution &lt;- grades |&gt; \n  count(grade)\n\nhead(grade_distribution)\n\n  grade    n\n1     A 1506\n2    A- 1381\n3    AU   27\n4     B  804\n5    B+ 1003\n6    B-  330",
    "crumbs": [
      "In-class Activities",
      "Factors"
    ]
  },
  {
    "objectID": "ica/Factors.html#exercises",
    "href": "ica/Factors.html#exercises",
    "title": "Factors",
    "section": "",
    "text": "Exercise 1: Changing Order\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\nCodegrade_distribution |&gt; \n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nExercise 2: Changing Factor Level Labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "In-class Activities",
      "Factors"
    ]
  },
  {
    "objectID": "ica/Factors.html#solutions",
    "href": "ica/Factors.html#solutions",
    "title": "Factors",
    "section": "Solutions",
    "text": "Solutions\n\nClick for Solutions\nExample 1: Default Orde\nThe categories are in alphabetical order, which isn’t meaningful here.\nExample 4: Re-ordering Levels using fct_relevel\n\nwe would have to:\n\nCalculate the typical Republican support in each state, e.g. using group_by() and summarize().\nWe’d then have to manually type out a meaningful order for 50 states! That’s a lot of typing and manual bookkeeping.\nExercise 1: Changing Order\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nExercise 2: Changing Factor Level Labels\n\nCodegrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n```",
    "crumbs": [
      "In-class Activities",
      "Factors"
    ]
  },
  {
    "objectID": "ica/strings.html",
    "href": "ica/strings.html",
    "title": "Strings",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "In-class Activities",
      "Strings"
    ]
  },
  {
    "objectID": "ica/strings.html#exercises",
    "href": "ica/strings.html#exercises",
    "title": "Strings",
    "section": "",
    "text": "Exercise 1: Time slots\nThe courses data includes actual data scraped from Mac’s class schedule. (Thanks to Prof Leslie Myint for the scraping code!!)\nIf you want to learn how to scrape data, take COMP/STAT 212, Intermediate Data Science! NOTE: For simplicity, I removed classes that had “TBA” for the days.\n\nCodelibrary(stringr)\nlibrary(tidyverse)\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\nUse our more familiar wrangling tools to warm up.\n\nCode# Construct a table that indicates the number of classes offered in each day/time slot\ncourses |&gt; \n  count(days, time) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n   days             time  n\n1 M W F 10:50 - 11:50 am 76\n2  T R   9:40 - 11:10 am 71\n3 M W F  9:40 - 10:40 am 68\n4 M W F   1:10 - 2:10 pm 66\n5  T R    3:00 - 4:30 pm 62\n6  T R    1:20 - 2:50 pm 59\n\nCode# Print only the 6 most popular time slots\n\n\nExercise 2: Prep the data\nSo that we can analyze it later, we want to wrangle the courses data:\n\nLet’s get some enrollment info:\n\nSplit avail_max into 2 separate variables: avail and max.\nUse avail and max to define a new variable called enroll. HINT: You’ll need as.numeric()\n\n\n\nSplit the course number into 3 separate variables: dept, number, and section. HINT: You can use separate() to split a variable into 3, not just 2 new variables.\n\nStore this as courses_clean so that you can use it later.\n\nCodecourses_clean &lt;- courses |&gt; \n  separate(avail_max,c(\"avail\",\"max\"),sep = \"/\" ) |&gt;\n  mutate(enroll = as.numeric(avail) - as.numeric(avail)) |&gt;\n  separate(number, c(\"dept\",\"number\",\"section\"))\n\nhead(courses_clean)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English    3   20      0\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4   16      0\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0   14      0\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery    3   25      0\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2   20      0\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason   -1   15      0\n\n\nExercise 3: Courses by department\nUsing courses_clean…\n\nCode# Identify the 6 departments that offered the most sections\ncourses_clean |&gt;\n  count(dept) |&gt;\n  arrange(desc(n)) |&gt;\n  head()\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\nCode# Identify the 6 departments with the longest average course titles\ncourses_clean |&gt;\n  mutate(length = str_length(name)) |&gt;\n  group_by(dept)|&gt;\n  summarize(avg_length = mean(length))|&gt;\n  arrange(desc(avg_length))|&gt;\n  head()\n\n# A tibble: 6 × 2\n  dept  avg_length\n  &lt;chr&gt;      &lt;dbl&gt;\n1 WGSS        46.3\n2 INTL        41.4\n3 EDUC        39.4\n4 MCST        39.4\n5 POLI        37.4\n6 AMST        37.3\n\n\nExercise 4: STAT courses\nPart a\nGet a subset of courses_clean that only includes courses taught by Alicia Johnson.\n\nCodecourses_clean |&gt; \n  filter(str_detect(instructor, \"Alicia Johnson\")) \n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson   -3   20      0\n2 THEATR 206 Alicia Johnson   -3   20      0\n3 THEATR 206 Alicia Johnson    2   20      0\n\n\nPart b\nCreate a new dataset from courses_clean, named stat, that only includes STAT sections. In this dataset:\n\n\nIn the course names:\n\nRemove “Introduction to” from any name.\nShorten “Statistical” to “Stat” where relevant.\n\n\nDefine a variable that records the start_time for the course.\nKeep only the number, name, start_time, enroll columns.\nThe result should have 19 rows and 4 columns.\n\n\nCodestat &lt;- courses_clean |&gt; \n  filter(dept == \"STAT\") |&gt; \n  mutate(name = str_replace(name, \"Introduction to \", \"\")) |&gt;\n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) |&gt; \n  mutate(start_time = str_sub(time, 1, 5)) |&gt; \n  select(number, name, start_time, enroll)\nstat\n\n   number                      name start_time enroll\n1     112              Data Science      3:00       0\n2     112              Data Science      9:40       0\n3     112              Data Science      1:20       0\n4     125              Epidemiology      12:00      0\n5     155             Stat Modeling      1:10       0\n6     155             Stat Modeling      9:40       0\n7     155             Stat Modeling      10:50      0\n8     155             Stat Modeling      3:30       0\n9     155             Stat Modeling      1:20       0\n10    155             Stat Modeling      3:00       0\n11    212 Intermediate Data Science      9:40       0\n12    212 Intermediate Data Science      1:20       0\n13    253     Stat Machine Learning      9:40       0\n14    253     Stat Machine Learning      1:20       0\n15    253     Stat Machine Learning      3:00       0\n16    354               Probability      3:00       0\n17    452           Correlated Data      9:40       0\n18    452           Correlated Data      1:20       0\n19    456  Projects in Data Science      9:40       0\n\nCodedim(stat)\n\n[1] 19  4\n\n\nExercise 5: More cleaning\nIn the next exercises, we’ll dig into enrollments. Let’s get the data ready for that analysis here. Make the following changes to the courses_clean data. Because they have different enrollment structures, and we don’t want to compare apples and oranges, remove the following:\n\nall sections in PE and INTD (interdisciplinary studies courses)\nall music ensembles and dance practicums, i.e. all MUSI and THDA classes with numbers less than 100. HINT: !(dept == \"MUSI\" & as.numeric(number) &lt; 100)\nall lab sections. Be careful which variable you use here. For example, you don’t want to search by “Lab” and accidentally eliminate courses with words such as “Labor”.\n\nSave the results as enrollments (don’t overwrite courses_clean).\nExercise 6: Enrollment & departments\nExplore enrollments by department. You decide what research questions to focus on. Use both visual and numerical summaries.\nExercise 7: Enrollment & faculty\nLet’s now explore enrollments by instructor. In doing so, we have to be cautious of cross-listed courses that are listed under multiple different departments. Uncomment the code lines in the chunk below for an example.\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\nCode# enrollments |&gt;\n#   filter(dept %in% c(\"STAT\", \"COMP\"), number == 112, section == \"01\")\n\n\nNotice that these are the exact same section! In order to not double count an instructor’s enrollments, we can keep only the courses that have distinct() combinations of days, time, instructor values. Uncomment the code lines in the chunk below.\n\nCode# enrollments_2 &lt;- enrollments |&gt; \n#   distinct(days, time, instructor, .keep_all = TRUE)\n\n# NOTE: By default this keeps the first department alphabetically\n# That's fine because we won't use this to analyze department enrollments!\n# enrollments_2 |&gt; \n#   filter(instructor == \"Brianna Heggeseth\", name == \"Introduction to Data Science\")\n\n\nNow, explore enrollments by instructor. You decide what research questions to focus on. Use both visual and numerical summaries.\nCAVEAT: The above code doesn’t deal with co-taught courses that have more than one instructor. Thus instructors that co-taught are recorded as a pair, and their co-taught enrollments aren’t added to their total enrollments. This is tough to get around with how the data were scraped as the instructor names are smushed together, not separated by a comma!\nOptional extra practice\n\nCode# Make a bar plot showing the number of night courses by day of the week\n# Use courses_clean\n\n\nDig Deeper: regex\nExample 4 gave 1 small example of a regular expression.\nThese are handy when we want process a string variable, but there’s no consistent pattern by which to do this. You must think about the structure of the string and how you can use regular expressions to capture the patterns you want (and exclude the patterns you don’t want).\nFor example, how would you describe the pattern of a 10-digit phone number? Limit yourself to just a US phone number for now.\n\nThe first 3 digits are the area code.\nThe next 3 digits are the exchange code.\nThe last 4 digits are the subscriber number.\n\nThus, a regular expression for a US phone number could be:\n\n\n[:digit:]{3}-[:digit:]{3}-[:digit:]{4} which limits you to XXX-XXX-XXXX pattern or\n\n\\\\([:digit:]{3}\\\\) [:digit:]{3}-[:digit:]{4} which limits you to (XXX) XXX-XXXX pattern or\n\n[:digit:]{3}\\\\.[:digit:]{3}\\\\.[:digit:]{4} which limits you to XXX.XXX.XXXX pattern\n\nThe following would include the three patterns above in addition to the XXXXXXXXXX pattern (no dashes or periods): - [\\\\(]*[:digit:]{3}[-.\\\\)]*[:digit:]{3}[-.]*[:digit:]{4}\nIn order to write a regular expression, you first need to consider what patterns you want to include and exclude.\nWork through the following examples, and the tutorial after them to learn about the syntax.\nEXAMPLES\n\nCode# Define some strings to play around with\nexample &lt;- \"The quick brown fox jumps over the lazy dog.\"\n\n\n\nCodestr_replace(example, \"quick\", \"really quick\")\n\n[1] \"The really quick brown fox jumps over the lazy dog.\"\n\n\n\nCodestr_replace_all(example, \"(fox|dog)\", \"****\") # | reads as OR\n\n[1] \"The quick brown **** jumps over the lazy ****.\"\n\n\n\nCodestr_replace_all(example, \"(fox|dog).\", \"****\") # \".\" for any character\n\n[1] \"The quick brown ****jumps over the lazy ****\"\n\n\n\nCodestr_replace_all(example, \"(fox|dog)\\\\.$\", \"****\") # at end of sentence only, \"\\\\.\" only for a period\n\n[1] \"The quick brown fox jumps over the lazy ****\"\n\n\n\nCodestr_replace_all(example, \"the\", \"a\") # case-sensitive only matches one\n\n[1] \"The quick brown fox jumps over a lazy dog.\"\n\n\n\nCodestr_replace_all(example, \"[Tt]he\", \"a\") # # will match either t or T; could also make \"a\" conditional on capitalization of t\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\nCodestr_replace_all(example, \"[Tt]he\", \"a\") # first match only\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\nCode# More examples\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\n\n# Store the examples in 1 place\nexamples &lt;- c(example, example2, example3)\n\n\n\nCodepat &lt;- \"[^aeiouAEIOU ]{3}\" # Regular expression for three straight consonants. Note that I've excluded spaces as well\n\nstr_detect(examples, pat) # TRUE/FALSE if it detects pattern\n\n[1]  TRUE  TRUE FALSE\n\n\n\nCodestr_subset(examples, pat) # Pulls out those that detects pattern\n\n[1] \"The quick brown fox jumps over the lazy dog.\"                                                                                                        \n[2] \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\n\n\n\nCodepat2 &lt;- \"[^aeiouAEIOU ][aeiouAEIOU]{2}[^aeiouAEIOU ]{1}\" # consonant followed by two vowels followed by a consonant\n\nstr_extract(example2, pat2) # extract first match\n\n[1] \"road\"\n\n\n\nCodestr_extract_all(example2, pat2, simplify = TRUE) # extract all matches\n\n     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  \n[1,] \"road\" \"wood\" \"coul\" \"tood\" \"look\" \"coul\"\n\n\nTUTORIAL\nTry out this interactive tutorial. Note that neither the tutorial nor regular expressions more generally are specific to R, but it still illustrates the main ideas of regular expressions.",
    "crumbs": [
      "In-class Activities",
      "Strings"
    ]
  },
  {
    "objectID": "ica/Data_Import.html",
    "href": "ica/Data_Import.html",
    "title": "Data Import",
    "section": "",
    "text": "Exercises\nSuppose our goal is to work with data on movie reviews, and that we’ve already gone through the work to find a dataset. The imdb_5000_messy.csv file is posted on Moodle. Let’s work with it!",
    "crumbs": [
      "In-class Activities",
      "Data Import"
    ]
  },
  {
    "objectID": "ica/Data_Import.html#exercises",
    "href": "ica/Data_Import.html#exercises",
    "title": "Data Import",
    "section": "",
    "text": "Exercise 1: Save Data Locally\nPart a\nOn your laptop:\n\nDownload the “imdb_5000_messy.csv” file from Moodle\nMove it to the data folder in your portfolio repository\nPart b\nHot tip: After saving your data file, it’s important to record appropriate citations and info in either a new qmd (eg: “imdb_5000_messy_README.qmd”) or in the qmd where you’ll analyze the data. These citations should include:\n\nthe data source, i.e. where you found the data\nthe data creator, i.e. who / what group collected the original data\npossibly a data codebook, i.e. descriptions of the data variables\n\nTo this end, check out where we originally got our IMDB data:\nhttps://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata\nAfter visiting that website, take some quick notes here on the data source and creator.\nExercise 2: Import Data to RStudio\nNow that we have a local copy of our data file, let’s get it into RStudio! Remember that this process depends on 2 things: the file type and location. Since our file type is a csv, we can import it using read_csv(). But we have to supply the file location through a file path. To this end, we can either use an absolute file path or a relative file path.\nPart a\nAn absolute file path describes the location of a file starting from the root or home directory. How we refer to the user root directory depends upon your machine:\n\nOn a Mac: ~\n\nOn Windows: typically C:\\\n\n.. means goes one level up!\n\nThen the complete file path to the IMDB data file in the data folder, depending on your machine an where you created your portfolio project, can be:\n\nOn a Mac: ~/Desktop/portfolio/data/imdb_5000_messy.csv\n\nOn Windows: C:\\Desktop\\portfolio\\data\\imdb_5000_messy.csv or C:\\\\Desktop\\\\portfolio\\\\data\\\\imdb_5000_messy.csv\n\n\nPutting this together, use read_csv() with the appropriate absolute file path to import your data into RStudio. Save this as imdb_messy.\n\nCodelibrary(tidyverse)\nimdb_messy &lt;- read_csv(\"~/Desktop/portfolio-srogrady/data/imdb_5000_messy.csv\")\n\n\nPart b\nAbsolute file paths can get really long, depending upon our number of sub-folders, and they should not be used when sharing code with other and instead relative file paths should be used. A relative file path describes the location of a file from the current “working directory”, i.e. where RStudio would currently look for on your computer. Check what your working directory is inside this qmd:\n\nCode# This should be the folder where you stored this qmd!\ngetwd()\n\n[1] \"/Users/shayleeogrady/Desktop/portfolio-srogrady/ica\"\n\n\nNext, check what the working directory is for the console by typing getwd() in the console. This is probably different, meaning that the relative file paths that will work in your qmd won’t work in the console! You can either exclusively work inside your qmd, or change the working directory in your console, by navigating to the following in the upper toolbar: Session &gt; Set Working Directory &gt; To Source File location.\nPart c\nAs a good practice, we created a data folder and saved our data file (imdb_5000_messy.csv) into.\nSince our .qmd analysis and .csv data live in the same project, we don’t have to write out absolute file paths that go all the way to the root directory. We can use relative file paths that start from where our code file exists to where the data file exist:\n\nOn a Mac: ../data/imdb_5000_messy.csv\n\nOn Windows: ..\\data\\imdb_5000_messy.csv or ..\\\\data\\\\imdb_5000_messy.csv\n\n\nNOTE: .. means go up one level in the file hierarchy, ie, go to the parent folder/directory.\nPutting this together, use read_csv() with the appropriate relative file path to import your data into RStudio. Save this as imdb_temp (temp for “temporary”). Convince yourself that this worked, i.e. you get the same dataset as imdb_messy.\n\nCodeimdb_temp &lt;- read_csv(\"../data/imdb_5000_messy.csv\")\n\n\n\n\n\n\n\n\nFile Paths\n\n\n\nAbsolute file paths should be used when referring to files hosed on the web, eg, https://mac-stat.github.io/data/kiva_partners2.csv. In all other instances, relative file paths should be used.\n\n\nPart d: OPTIONAL\nSometimes, we don’t want to import the entire dataset. For example, we might want to…\n\nskips some rows, eg, if they’re just “filler”\nonly import the first “n” rows, eg, if the dataset is really large\nonly import a random subset of “n” rows, eg, if the dataset is really large\n\nThe “data import cheat sheet” at the top of this qmd, or Google, are handy resources here. As one example…\n\nCode# Try importing only the first 5 rows\n read_csv(\"../data/imdb_5000_messy.csv\", n_max = 5)\n\n# A tibble: 5 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1     1 Color James Cameron                        723      178\n2     2 Color Gore Verbinski                       302      169\n3     3 Color Sam Mendes                           602      148\n4     4 Color Christopher Nolan                    813      164\n5     5 &lt;NA&gt;  Doug Walker                           NA       NA\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\nExercise 3: Check Data\nAfter importing new data into RStudio, you MUST do some quick checks of the data. Here are two first steps that are especially useful.\nPart a\nOpen imdb_messy in the spreadsheet-like viewer by typing View(imdb_messy) in the console. Sort this “spreadsheet” by different variables by clicking on the arrows next to the variable names. Do you notice anything unexpected?\nPart b\nDo a quick summary() of each variable in the dataset. One way to do this is below:\n\nCodeimdb_messy |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;  # convert characters to factors in order to summarize\n  summary()\n\n      ...1                  color               director_name \n Min.   :   1   B&W            :  10   Steven Spielberg:  26  \n 1st Qu.:1262   Black and White: 199   Woody Allen     :  22  \n Median :2522   color          :  30   Clint Eastwood  :  20  \n Mean   :2522   Color          :4755   Martin Scorsese :  20  \n 3rd Qu.:3782   COLOR          :  30   Ridley Scott    :  17  \n Max.   :5043   NA's           :  19   (Other)         :4834  \n                                       NA's            : 104  \n num_critic_for_reviews    duration     director_facebook_likes\n Min.   :  1.0          Min.   :  7.0   Min.   :    0.0        \n 1st Qu.: 50.0          1st Qu.: 93.0   1st Qu.:    7.0        \n Median :110.0          Median :103.0   Median :   49.0        \n Mean   :140.2          Mean   :107.2   Mean   :  686.5        \n 3rd Qu.:195.0          3rd Qu.:118.0   3rd Qu.:  194.5        \n Max.   :813.0          Max.   :511.0   Max.   :23000.0        \n NA's   :50             NA's   :15      NA's   :104            \n actor_3_facebook_likes          actor_2_name  actor_1_facebook_likes\n Min.   :    0.0        Morgan Freeman :  20   Min.   :     0        \n 1st Qu.:  133.0        Charlize Theron:  15   1st Qu.:   614        \n Median :  371.5        Brad Pitt      :  14   Median :   988        \n Mean   :  645.0        James Franco   :  11   Mean   :  6560        \n 3rd Qu.:  636.0        Meryl Streep   :  11   3rd Qu.: 11000        \n Max.   :23000.0        (Other)        :4959   Max.   :640000        \n NA's   :23             NA's           :  13   NA's   :7             \n     gross                            genres             actor_1_name \n Min.   :      162   Drama               : 236   Robert De Niro:  49  \n 1st Qu.:  5340988   Comedy              : 209   Johnny Depp   :  41  \n Median : 25517500   Comedy|Drama        : 191   Nicolas Cage  :  33  \n Mean   : 48468408   Comedy|Drama|Romance: 187   J.K. Simmons  :  31  \n 3rd Qu.: 62309438   Comedy|Romance      : 158   Bruce Willis  :  30  \n Max.   :760505847   Drama|Romance       : 152   (Other)       :4852  \n NA's   :884         (Other)             :3910   NA's          :   7  \n                    movie_title   num_voted_users   cast_total_facebook_likes\n Ben-Hur                  :   3   Min.   :      5   Min.   :     0           \n Halloween                :   3   1st Qu.:   8594   1st Qu.:  1411           \n Home                     :   3   Median :  34359   Median :  3090           \n King Kong                :   3   Mean   :  83668   Mean   :  9699           \n Pan                      :   3   3rd Qu.:  96309   3rd Qu.: 13756           \n The Fast and the Furious :   3   Max.   :1689764   Max.   :656730           \n (Other)                  :5025                                              \n         actor_3_name  facenumber_in_poster\n Ben Mendelsohn:   8   Min.   : 0.000      \n John Heard    :   8   1st Qu.: 0.000      \n Steve Coogan  :   8   Median : 1.000      \n Anne Hathaway :   7   Mean   : 1.371      \n Jon Gries     :   7   3rd Qu.: 2.000      \n (Other)       :4982   Max.   :43.000      \n NA's          :  23   NA's   :13          \n                                                                           plot_keywords \n based on novel                                                                   :   4  \n 1940s|child hero|fantasy world|orphan|reference to peter pan                     :   3  \n alien friendship|alien invasion|australia|flying car|mother daughter relationship:   3  \n animal name in title|ape abducts a woman|gorilla|island|king kong                :   3  \n assistant|experiment|frankenstein|medical student|scientist                      :   3  \n (Other)                                                                          :4874  \n NA's                                                                             : 153  \n                                             movie_imdb_link\n http://www.imdb.com/title/tt0077651/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0232500/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0360717/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt1976009/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2224026/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2638144/?ref_=fn_tt_tt_1:   3  \n (Other)                                             :5025  \n num_user_for_reviews     language       country       content_rating\n Min.   :   1.0       English :4704   USA    :3807   R        :2118  \n 1st Qu.:  65.0       French  :  73   UK     : 448   PG-13    :1461  \n Median : 156.0       Spanish :  40   France : 154   PG       : 701  \n Mean   : 272.8       Hindi   :  28   Canada : 126   Not Rated: 116  \n 3rd Qu.: 326.0       Mandarin:  26   Germany:  97   G        : 112  \n Max.   :5060.0       (Other) : 160   (Other): 406   (Other)  : 232  \n NA's   :21           NA's    :  12   NA's   :   5   NA's     : 303  \n     budget            title_year   actor_2_facebook_likes   imdb_score   \n Min.   :2.180e+02   Min.   :1916   Min.   :     0         Min.   :1.600  \n 1st Qu.:6.000e+06   1st Qu.:1999   1st Qu.:   281         1st Qu.:5.800  \n Median :2.000e+07   Median :2005   Median :   595         Median :6.600  \n Mean   :3.975e+07   Mean   :2002   Mean   :  1652         Mean   :6.442  \n 3rd Qu.:4.500e+07   3rd Qu.:2011   3rd Qu.:   918         3rd Qu.:7.200  \n Max.   :1.222e+10   Max.   :2016   Max.   :137000         Max.   :9.500  \n NA's   :492         NA's   :108    NA's   :13                            \n  aspect_ratio   movie_facebook_likes\n Min.   : 1.18   Min.   :     0      \n 1st Qu.: 1.85   1st Qu.:     0      \n Median : 2.35   Median :   166      \n Mean   : 2.22   Mean   :  7526      \n 3rd Qu.: 2.35   3rd Qu.:  3000      \n Max.   :16.00   Max.   :349000      \n NA's   :329                         \n\n\nFollow-up:\n\nWhat type of info is provided on quantitative variables? min,median,max,q1,q2,mean\nWhat type of info is provided on categorical variables? the specifics and the count of them.\nWhat stands out to you in these summaries? Is there anything you’d need to clean before using this data? yes there are lots of NAs would need to clean that.\nExercise 4: Clean Data: Factor Variables 1\nIf you didn’t already in Exercise 3, check out the color variable in the imdb_messy dataset.\n\nWhat’s goofy about this / what do we need to fix?\nMore specifically, what different categories does the color variable take, and how many movies fall into each of these categories?\n\n\nCodeimdb_messy |&gt; \n  count(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\n\nExercise 5: Clean Data: Factor Variables 2\nWhen working with categorical variables like color, the categories must be “clean”, i.e. consistent and in the correct format. Let’s make that happen.\nPart a\nWe could open the .csv file in, say, Excel or Google sheets, clean up the color variable, save a clean copy, and then reimport that into RStudio. BUT that would be the wrong thing to do. Why is it important to use R code, which we then save inside this qmd, to clean our data?\nPart b\nLet’s use R code to change the color variable so that it appropriately combines the various categories into only 2: Color and Black_White. We’ve learned a couple sets of string-related tools that could be handy here. First, starting with the imdb_messy data, change the color variable using one of the functions we learned in the Factors lesson.\nfct_relevel(), fct_recode(), fct_reorder()\nStore your results in imdb_temp (don’t overwrite imdb_messy). To check your work, print out a count() table of the color variable in imdb_temp.\n\nCodeimdb_temp &lt;- imdb_messy |&gt; \n  mutate(color = fct_recode(color,\n                            \"Color\" = \"COLOR\",\n                            \"Color\" = \"color\",\n                            \"Black_White\" = \"B&W\",\n                            \"Black_White\" = \"Black and White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;fct&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nPart c\nRepeat Part b using one of our string functions from the String lesson:\nstr_replace(), str_replace_all(), str_to_lower(), str_sub(), str_length(), str_detect()\n\nCodeimdb_temp &lt;- imdb_messy |&gt; \n  mutate(color = str_replace(color, \"COLOR\", \"Color\"),\n         color = str_replace(color, \"color\", \"Color\"),\n         color = str_replace(color, \"B&W\", \"Black_White\"),\n         color = str_replace(color, \"Black and White\", \"Black_White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;chr&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nExercise 6: Clean Data: Missing Data 1\nThroughout these exercises, you’ve probably noticed that there’s a bunch of missing data. This is encoded as NA (not available) in R. There are a few questions to address about missing data:\n\n\nHow many values are missing data? What’s the volume of the missingness?\n\nWhy are some values missing?\n\nWhat should we do about the missing values?\n\nLet’s consider the first 2 questions in this exercise.\nPart a\nAs a first step, let’s simply understand the volume of NAs. Specifically:\n\nCode# Count the total number of rows in imdb_messy\n#___(imdb_messy)\n\n# Then count the number of NAs in each column\n#colSums(is.na(imdb_messy))\n\n# Then count the number of NAs in a specific column\n#imdb_messy |&gt; \n#  ___(___(is.na(actor_1_facebook_likes)))\n\n\nPart b\nAs a second step, let’s think about why some values are missing. Study the individual observations with NAs carefully. Why do you think they are missing? Are certain films more likely to have more NAs than others?\nPart c\nConsider a more specific example. Obtain a dataset of movies that are missing data on actor_1_facebook_likes. Then explain why you think there are NAs. HINT: is.na(___)\nExercise 7: Clean Data: Missing Data 2\nNext, let’s think about what to do about the missing values. There is no perfect or universal approach here. Rather, we must think carefully about…\n\nWhy the values are missing?\nWhat we want to do with our data?\nWhat is the impact of removing or replacing missing data on our work / conclusions?\n\nPart a\nCalculate the average duration of a film. THINK: How can we deal with the NA’s?\nFollow-up:\nHow are the NAs dealt with here? Did we have to create and save a new dataset in order to do this analysis?\nPart b\nTry out the drop_na() function:\n\nCode# imdb_temp &lt;- drop_na(imdb_messy)\n\n\nFollow-up questions:\n\nWhat did drop_na() do? How many data points are left?\nIn what situations might this function be a good idea?\nIn what situations might this function be a bad idea?\nPart c\ndrop_na() removes data points that have any NA values, even if we don’t care about the variable(s) for which data is missing. This can result in losing a lot of data points that do have data on the variables we actually care about! For example, suppose we only want to explore the relationship between film duration and whether it’s in color. Check out a plot:\n\nCode# ggplot(imdb_messy, aes(x = duration, fill = color)) + \n#   geom_density()\n\n\nFollow-up:\n\nCreate a new dataset with only and all movies that have complete info on duration and color. HINT: You could use !is.na(___) or drop_na() (differently than above)\nUse this new dataset to create a new and improved plot.\nHow many movies remain in your new dataset? Hence why this is better than using the dataset from part b?\nPart d\nIn some cases, missing data is more non-data than unknown data. For example, the films with NAs for actor_1_facebook_likes actually have 0 Facebook likes–they don’t even have actors! In these cases, we can replace the NAs with a 0. Use the replace_na() function to create a new dataset (imdb_temp) that replaces the NAs in actor_1_facebook_likes with 0. You’ll have to check out the help file for this function.\nExercise 8: New Data + Projects\nLet’s practice the above ideas while also planting some seeds for the course project. Each group will pick and analyze their own dataset. The people you’re sitting with today aren’t necessarily your project groups! BUT do some brainstorming together:\n\nShare with each other: What are some personal hobbies or passions or things you’ve been thinking about or things you’d like to learn more about? Don’t think too hard about this! Just share what’s at the top of mind today.\nEach individual: Find a dataset online that’s related to one of the topics you shared in the above prompt.\nDiscuss what data you found with your group!\nLoad the data into RStudio, perform some basic checks, and perform some preliminary cleaning, as necessary.",
    "crumbs": [
      "In-class Activities",
      "Data Import"
    ]
  },
  {
    "objectID": "ica/Data_Import.html#solutions",
    "href": "ica/Data_Import.html#solutions",
    "title": "Data Import",
    "section": "Solutions",
    "text": "Solutions\n\nClick for Solutions\nExercise 3: Check Data\nPart b\nThere are many NA’s, the color variable is goofy…\n\nCodeimdb_messy |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;  # convert characters to factors in order to summarize\n  summary()\n\n      ...1                  color               director_name \n Min.   :   1   B&W            :  10   Steven Spielberg:  26  \n 1st Qu.:1262   Black and White: 199   Woody Allen     :  22  \n Median :2522   color          :  30   Clint Eastwood  :  20  \n Mean   :2522   Color          :4755   Martin Scorsese :  20  \n 3rd Qu.:3782   COLOR          :  30   Ridley Scott    :  17  \n Max.   :5043   NA's           :  19   (Other)         :4834  \n                                       NA's            : 104  \n num_critic_for_reviews    duration     director_facebook_likes\n Min.   :  1.0          Min.   :  7.0   Min.   :    0.0        \n 1st Qu.: 50.0          1st Qu.: 93.0   1st Qu.:    7.0        \n Median :110.0          Median :103.0   Median :   49.0        \n Mean   :140.2          Mean   :107.2   Mean   :  686.5        \n 3rd Qu.:195.0          3rd Qu.:118.0   3rd Qu.:  194.5        \n Max.   :813.0          Max.   :511.0   Max.   :23000.0        \n NA's   :50             NA's   :15      NA's   :104            \n actor_3_facebook_likes          actor_2_name  actor_1_facebook_likes\n Min.   :    0.0        Morgan Freeman :  20   Min.   :     0        \n 1st Qu.:  133.0        Charlize Theron:  15   1st Qu.:   614        \n Median :  371.5        Brad Pitt      :  14   Median :   988        \n Mean   :  645.0        James Franco   :  11   Mean   :  6560        \n 3rd Qu.:  636.0        Meryl Streep   :  11   3rd Qu.: 11000        \n Max.   :23000.0        (Other)        :4959   Max.   :640000        \n NA's   :23             NA's           :  13   NA's   :7             \n     gross                            genres             actor_1_name \n Min.   :      162   Drama               : 236   Robert De Niro:  49  \n 1st Qu.:  5340988   Comedy              : 209   Johnny Depp   :  41  \n Median : 25517500   Comedy|Drama        : 191   Nicolas Cage  :  33  \n Mean   : 48468408   Comedy|Drama|Romance: 187   J.K. Simmons  :  31  \n 3rd Qu.: 62309438   Comedy|Romance      : 158   Bruce Willis  :  30  \n Max.   :760505847   Drama|Romance       : 152   (Other)       :4852  \n NA's   :884         (Other)             :3910   NA's          :   7  \n                    movie_title   num_voted_users   cast_total_facebook_likes\n Ben-Hur                  :   3   Min.   :      5   Min.   :     0           \n Halloween                :   3   1st Qu.:   8594   1st Qu.:  1411           \n Home                     :   3   Median :  34359   Median :  3090           \n King Kong                :   3   Mean   :  83668   Mean   :  9699           \n Pan                      :   3   3rd Qu.:  96309   3rd Qu.: 13756           \n The Fast and the Furious :   3   Max.   :1689764   Max.   :656730           \n (Other)                  :5025                                              \n         actor_3_name  facenumber_in_poster\n Ben Mendelsohn:   8   Min.   : 0.000      \n John Heard    :   8   1st Qu.: 0.000      \n Steve Coogan  :   8   Median : 1.000      \n Anne Hathaway :   7   Mean   : 1.371      \n Jon Gries     :   7   3rd Qu.: 2.000      \n (Other)       :4982   Max.   :43.000      \n NA's          :  23   NA's   :13          \n                                                                           plot_keywords \n based on novel                                                                   :   4  \n 1940s|child hero|fantasy world|orphan|reference to peter pan                     :   3  \n alien friendship|alien invasion|australia|flying car|mother daughter relationship:   3  \n animal name in title|ape abducts a woman|gorilla|island|king kong                :   3  \n assistant|experiment|frankenstein|medical student|scientist                      :   3  \n (Other)                                                                          :4874  \n NA's                                                                             : 153  \n                                             movie_imdb_link\n http://www.imdb.com/title/tt0077651/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0232500/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0360717/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt1976009/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2224026/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2638144/?ref_=fn_tt_tt_1:   3  \n (Other)                                             :5025  \n num_user_for_reviews     language       country       content_rating\n Min.   :   1.0       English :4704   USA    :3807   R        :2118  \n 1st Qu.:  65.0       French  :  73   UK     : 448   PG-13    :1461  \n Median : 156.0       Spanish :  40   France : 154   PG       : 701  \n Mean   : 272.8       Hindi   :  28   Canada : 126   Not Rated: 116  \n 3rd Qu.: 326.0       Mandarin:  26   Germany:  97   G        : 112  \n Max.   :5060.0       (Other) : 160   (Other): 406   (Other)  : 232  \n NA's   :21           NA's    :  12   NA's   :   5   NA's     : 303  \n     budget            title_year   actor_2_facebook_likes   imdb_score   \n Min.   :2.180e+02   Min.   :1916   Min.   :     0         Min.   :1.600  \n 1st Qu.:6.000e+06   1st Qu.:1999   1st Qu.:   281         1st Qu.:5.800  \n Median :2.000e+07   Median :2005   Median :   595         Median :6.600  \n Mean   :3.975e+07   Mean   :2002   Mean   :  1652         Mean   :6.442  \n 3rd Qu.:4.500e+07   3rd Qu.:2011   3rd Qu.:   918         3rd Qu.:7.200  \n Max.   :1.222e+10   Max.   :2016   Max.   :137000         Max.   :9.500  \n NA's   :492         NA's   :108    NA's   :13                            \n  aspect_ratio   movie_facebook_likes\n Min.   : 1.18   Min.   :     0      \n 1st Qu.: 1.85   1st Qu.:     0      \n Median : 2.35   Median :   166      \n Mean   : 2.22   Mean   :  7526      \n 3rd Qu.: 2.35   3rd Qu.:  3000      \n Max.   :16.00   Max.   :349000      \n NA's   :329                         \n\n\nExercise 4: Clean Data: Factor Variables 1\n\nCodeimdb_messy |&gt; \n  count(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\n\nExercise 5: Clean Data: Factor Variables 2\nPart a\nThat wouldn’t be reproducible. It’s important to log all steps in our data cleaning, so that we and others know and could reproduce those steps.\nPart b\n\nCodeimdb_temp &lt;- imdb_messy |&gt; \n  mutate(color = fct_recode(color,\n                            \"Color\" = \"COLOR\",\n                            \"Color\" = \"color\",\n                            \"Black_White\" = \"B&W\",\n                            \"Black_White\" = \"Black and White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;fct&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nPart c\n\nCodeimdb_temp &lt;- imdb_messy |&gt; \n  mutate(color = str_replace(color, \"COLOR\", \"Color\"),\n         color = str_replace(color, \"color\", \"Color\"),\n         color = str_replace(color, \"B&W\", \"Black_White\"),\n         color = str_replace(color, \"Black and White\", \"Black_White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;chr&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nExercise 6: Clean Data: Missing Data 1\nPart a\n\nCode# Count the total number of rows in imdb_messy\nnrow(imdb_messy)\n\n[1] 5043\n\nCode# Then count the number of NAs in each column\ncolSums(is.na(imdb_messy))\n\n                     ...1                     color             director_name \n                        0                        19                       104 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                       50                        15                       104 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                       23                        13                         7 \n                    gross                    genres              actor_1_name \n                      884                         0                         7 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                       23                        13                       153 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                        21                        12 \n                  country            content_rating                    budget \n                        5                       303                       492 \n               title_year    actor_2_facebook_likes                imdb_score \n                      108                        13                         0 \n             aspect_ratio      movie_facebook_likes \n                      329                         0 \n\n\nPart c\nThese are all documentaries that don’t have any actors.\n\nCodeimdb_messy |&gt; \n  filter(is.na(actor_1_facebook_likes))\n\n# A tibble: 7 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1  4503 Color Léa Pool                              23       97\n2  4520 Color Harry Gantz                           12      105\n3  4721 Color U. Roberto Romano                      3       80\n4  4838 Color Pan Nalin                             15      102\n5  4946 Color Amal Al-Agroobi                       NA       62\n6  4947 Color Andrew Berends                        12       90\n7  4991 Color Jem Cohen                             12      111\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\nExercise 7: Clean: Missing Data 2\nPart a\n\nCodeimdb_messy |&gt; \n  summarize(mean(duration, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(duration, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                           107.\n\n\nFollow-up:\nWe just remove the NAs from the calculation. No need to entirely remove the related movies from the dataset.\nPart b\nThis gets rid of any movie with any NAs. There are only 3756 movies left! This approach is heavy-handed. It’s typically only a good idea when we need complete info on every variable for every part of our analysis.\n\nCodeimdb_temp &lt;- drop_na(imdb_messy)\nnrow(imdb_temp)\n\n[1] 3756\n\nCodecolSums(is.na(imdb_temp))\n\n                     ...1                     color             director_name \n                        0                         0                         0 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                        0                         0                         0 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                        0                         0                         0 \n                    gross                    genres              actor_1_name \n                        0                         0                         0 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                        0                         0                         0 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                         0                         0 \n                  country            content_rating                    budget \n                        0                         0                         0 \n               title_year    actor_2_facebook_likes                imdb_score \n                        0                         0                         0 \n             aspect_ratio      movie_facebook_likes \n                        0                         0 \n\n\nPart c\n\nCodeggplot(imdb_messy, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\n\n\n\nWe keep most of the movies!\n\nCode# Approach 1\nimdb_temp &lt;- imdb_messy |&gt; \n  select(duration, color) |&gt; \n  drop_na()\ndim(imdb_temp)\n\n[1] 5010    2\n\nCode# Approach 2\nimdb_temp &lt;- imdb_messy |&gt; \n  filter(!is.na(duration), !is.na(color))\ndim(imdb_temp)\n\n[1] 5010   29\n\nCode# Plot\nggplot(imdb_temp, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\n\n\n\nPart d\n\nCodeimdb_temp &lt;- imdb_messy |&gt; \n  mutate(actor_1_facebook_likes =\n         replace_na(actor_1_facebook_likes, 0))\n\nimdb_temp |&gt; \n  summarize(sum(is.na(actor_1_facebook_likes)))\n\n# A tibble: 1 × 1\n  `sum(is.na(actor_1_facebook_likes))`\n                                 &lt;int&gt;\n1                                    0",
    "crumbs": [
      "In-class Activities",
      "Data Import"
    ]
  },
  {
    "objectID": "test/Exam1.html",
    "href": "test/Exam1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Grand Research Question :\nWhat does the consumption of each food category in each country look like ?\n\nCodelibrary(tidyverse)\nlibrary(readr)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n\n\nCodelibrary(tidytuesdayR)\ntuesdata &lt;- tt_load('2020-02-18')\nfc &lt;-tuesdata$food_consumption\n\n\nFirst let’s look at our data :\n\nCodehead(fc)\n\n# A tibble: 6 × 4\n  country   food_category consumption co2_emmission\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Argentina Pork                10.5          37.2 \n2 Argentina Poultry             38.7          41.5 \n3 Argentina Beef                55.5        1712   \n4 Argentina Lamb & Goat          1.56         54.6 \n5 Argentina Fish                 4.36          6.96\n6 Argentina Eggs                11.4          10.5 \n\n\n\nCodestr(fc)\n\nspc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country      : chr [1:1430] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ food_category: chr [1:1430] \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n $ consumption  : num [1:1430] 10.51 38.66 55.48 1.56 4.36 ...\n $ co2_emmission: num [1:1430] 37.2 41.53 1712 54.63 6.96 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   food_category = col_character(),\n  ..   consumption = col_double(),\n  ..   co2_emmission = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nCodenrow(fc)\n\n[1] 1430\n\n\n\nCodedim(fc)\n\n[1] 1430    4\n\n\n\nCodetail(fc)\n\n# A tibble: 6 × 4\n  country    food_category            consumption co2_emmission\n  &lt;chr&gt;      &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n1 Bangladesh Eggs                            2.08          1.91\n2 Bangladesh Milk - inc. cheese             21.9          31.2 \n3 Bangladesh Wheat and Wheat Products       17.5           3.33\n4 Bangladesh Rice                          172.          220.  \n5 Bangladesh Soybeans                        0.61          0.27\n6 Bangladesh Nuts inc. Peanut Butter         0.72          1.27\n\n\n\nCodehead(fc,22)\n\n# A tibble: 22 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 12 more rows\n\nCodetail(fc,22)\n\n# A tibble: 22 × 4\n   country food_category            consumption co2_emmission\n   &lt;chr&gt;   &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Liberia Pork                            4.01         14.2 \n 2 Liberia Poultry                         8.91          9.57\n 3 Liberia Beef                            0.78         24.1 \n 4 Liberia Lamb & Goat                     0.48         16.8 \n 5 Liberia Fish                            4.13          6.59\n 6 Liberia Eggs                            2.05          1.88\n 7 Liberia Milk - inc. cheese              3.04          4.33\n 8 Liberia Wheat and Wheat Products       11.0           2.09\n 9 Liberia Rice                           94.8         121.  \n10 Liberia Soybeans                        0.63          0.28\n# ℹ 12 more rows\n\n\nThe Answer :\nNot a very good visualization as we need to clean and wrangle our data more (see Exam 2)\n\nCodene_countries(returnclass = \"sf\") |&gt;\n  select(name) |&gt;\n  mutate(name = ifelse(name == \"United States of America\", \"USA\", name)) |&gt;\n  mutate(name = ifelse(name == \"Bosnia and Herz.\", \"Bosnia and Herzegovina\", name)) |&gt;\n  mutate(name = ifelse(name == \"Czechia\", \"Czech Republic\", name)) |&gt;\n  mutate(name = ifelse(name == \"Taiwan\", \"Taiwan. ROC\", name)) |&gt;\n  left_join(\n    fc |&gt;\n      select(-co2_emmission) |&gt;\n      group_by(food_category) |&gt;\n      mutate(consumption = (consumption - mean(consumption)) / sd(consumption)),\n    join_by(name == country)\n  ) |&gt;\n  ggplot(aes(x = name, y = consumption, color = name)) +\n  geom_point() +\n  facet_wrap(~food_category) +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  labs(x = \"Country\", y = \"Standardized Consumption\")",
    "crumbs": [
      "Exams",
      "Exam 1"
    ]
  },
  {
    "objectID": "test/Exam2.html",
    "href": "test/Exam2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Continued Exploration :\nWhat does the consumption of each food category in each country look like ?\n\nCodelibrary(tidyverse)\nlibrary(readr)\nlibrary(rnaturalearth)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nCodelibrary(tidytuesdayR)\ntuesdata &lt;- tt_load('2020-02-18')\nfc &lt;-tuesdata$food_consumption\n\n\n\nCodestr(fc)\n\nspc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country      : chr [1:1430] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ food_category: chr [1:1430] \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n $ consumption  : num [1:1430] 10.51 38.66 55.48 1.56 4.36 ...\n $ co2_emmission: num [1:1430] 37.2 41.53 1712 54.63 6.96 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   food_category = col_character(),\n  ..   consumption = col_double(),\n  ..   co2_emmission = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nCodehead(fc,22)\n\n# A tibble: 22 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 12 more rows\n\n\n\nCodefc |&gt;\n  select(food_category)|&gt;\n  distinct(food_category)\n\n# A tibble: 11 × 1\n   food_category           \n   &lt;chr&gt;                   \n 1 Pork                    \n 2 Poultry                 \n 3 Beef                    \n 4 Lamb & Goat             \n 5 Fish                    \n 6 Eggs                    \n 7 Milk - inc. cheese      \n 8 Wheat and Wheat Products\n 9 Rice                    \n10 Soybeans                \n11 Nuts inc. Peanut Butter \n\n\n\nCodelibrary(forcats)\n\nfcc &lt;- fc |&gt;\n  mutate(food_category = fct_recode(food_category,\n        \"Lamb\" = 'Lamb & Goat', \n        \"Dairy\"= 'Milk - inc. cheese',\n        \"Wheat\"='Wheat and Wheat Products',\n        \"Nuts\"='Nuts inc. Peanut Butter'))\n\n\n\nCodefcc |&gt;\n  select(food_category)|&gt;\n  distinct(food_category)\n\n# A tibble: 11 × 1\n   food_category\n   &lt;fct&gt;        \n 1 Pork         \n 2 Poultry      \n 3 Beef         \n 4 Lamb         \n 5 Fish         \n 6 Eggs         \n 7 Dairy        \n 8 Wheat        \n 9 Rice         \n10 Soybeans     \n11 Nuts         \n\n\n\nCodetop5 &lt;- fcc |&gt;\n  group_by(country)|&gt;\n  summarise(total_consumption = sum(consumption, na.rm = TRUE))|&gt;\n  arrange(desc(total_consumption))|&gt;\n  slice_head(n=5)\n\n\n\nCode  ggplot(top5, aes(x = reorder(country, -total_consumption), y = total_consumption))+\n  geom_col()+\n  labs(title = \"Top 5 Countries by Total Food Consumption\", x = \"Country\", y = \"Total Consumption\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nCodetop5_each_food &lt;- fcc |&gt;\n  group_by(food_category) |&gt;\n  arrange(desc(consumption)) |&gt;\n  slice_head(n = 5) |&gt;\n  select(food_category, country, consumption)\n\n\n\nCodeggplot(top5_each_food, aes(x = reorder(country, consumption), y = consumption, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ food_category, scales = \"free_y\") +\n  labs(\n    title = \"Top 5 Countries by Consumption per Food Category\",\n    x = \"Country\",\n    y = \"Consumption\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCodelibrary(viridis)\n\n#| fig-width: 22 \n#| fig-height: 11\n\nne_countries(returnclass = \"sf\") |&gt;\n  select(name, geometry) |&gt;\n  mutate(name = ifelse(name == \"United States of America\", \"USA\", name)) |&gt;\n  mutate(name = ifelse(name == \"Bosnia and Herz.\", \"Bosnia and Herzegovina\", name)) |&gt;\n  mutate(name = ifelse(name == \"Czechia\", \"Czech Republic\", name)) |&gt;\n  mutate(name = ifelse(name == \"Taiwan\", \"Taiwan. ROC\", name)) |&gt;\n  left_join(\n    fcc |&gt;\n      select(-co2_emmission) |&gt;\n      group_by(food_category) |&gt;\n      mutate(consumption = (consumption - mean(consumption)) / sd(consumption)),\n    join_by(name == country)\n  ) |&gt;\n  pivot_wider(names_from = food_category, values_from = consumption) |&gt;\n  select(-\"NA\") |&gt;\n  pivot_longer(\n    cols = c(-name, -geometry),\n    names_to = \"food_category\",\n    values_to = \"consumption\"\n  ) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = consumption)) +\n  scale_fill_viridis_c(option = \"D\")+\n  facet_wrap(~food_category) +\n  theme(legend.position = \"bottom\")+\n  theme_classic()+\n  labs(title = \"Choropleth Map of Consumption for Each Food by Country\")",
    "crumbs": [
      "Exams",
      "Exam 2"
    ]
  }
]